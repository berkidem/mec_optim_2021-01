{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Block 8: Discrete choice estimation</center>\n",
    "### <center>Alfred Galichon (NYU & Sciences Po)</center>\n",
    "## <center>'math+econ+code' masterclass on optimal transport and economic applications</center>\n",
    "<center>© 2018-2021 by Alfred Galichon. Past and present support from NSF grant DMS-1716489, ERC grant CoG-866274, and contributions by Jules Baudet, Pauline Corblet, Gregory Dannay, and James Nesbit are acknowledged.</center>\n",
    "\n",
    "#### <center>With python code examples</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* Savage, L. (1951). The theory of statistical decision. JASA.\n",
    "* Bonnet, Fougère, Galichon, Poulhès (2021). Minimax estimation of hedonic models. Preprint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the libraries\n",
    "\n",
    "First, let's load the libraries we shall need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import string as str\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import scipy.sparse as spr\n",
    "\n",
    "from scipy import optimize, special\n",
    "import gurobipy as grb\n",
    "\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "#from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our data\n",
    "We will go back to the dataset of Greene and Hensher (1997). As a reminder, 210 individuals are surveyed about their choice of travel mode between Sydney, Canberra and Melbourne, and the various costs (time and money) associated with each alternative. Therefore there are 840 = 4 x 210 observations, which we can stack into `travelmodedataset` a 3 dimensional array whose dimensions are mode,individual,dummy for choice+covariates.\n",
    "\n",
    "Let's load the dataset and represent it conveniently in a similar fashion as in block 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thepath = os.path.join(os.getcwd(),'..')\n",
    "travelmode =  pd.read_csv(os.path.join(thepath,'data_mec_optim/demand_travelmode/travelmodedata.csv'))\n",
    "\n",
    "travelmode['choice'] = np.where(travelmode['choice'] =='yes' , 1, 0)\n",
    "\n",
    "nobs = travelmode.shape[0]\n",
    "ncols = travelmode.shape[1]\n",
    "nbchoices = 4\n",
    "ninds = int(nobs/nbchoices)\n",
    "\n",
    "muhat_i_y = travelmode['choice'].values.reshape(ninds,nbchoices).T\n",
    "muhat_iy = muhat_i_y.flatten()\n",
    "\n",
    "muhat_i_y = travelmode['choice'].values.reshape(ninds,4).T\n",
    "muhat_iy = muhat_i_y.flatten()\n",
    "\n",
    "s_y = travelmode.groupby(['mode']).mean()['choice'].to_frame().sort_index()\n",
    "\n",
    "def two_d(X):\n",
    "    return np.reshape(X,(X.size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation with no observable heterogeneity\n",
    "\n",
    "Start with assuming that there is no observable heterogeneity, so the only observation we have at hand are the aggregate market shares $s_y$. Hence the systematic utility will be the same for every agent. However, we wish to write a parametric model for it, namely assume a knwon parametric form for the dependence of $U_y$ with respect to various observed characteristics associated with $y$.\n",
    "\n",
    "Assume then that the utilities are parameterized as follows: $U = \\Phi \\beta$ where $\\beta\\in\\mathbb{R}^{p}$ is a parameter, and $\\Phi$ is a $\\left\\vert \\mathcal{Y}\\right\\vert \\times p$ matrix.\n",
    "\n",
    "The log-likelihood function is given by\n",
    "\n",
    "\\begin{align*}\n",
    "l\\left(  \\beta\\right)  =N\\sum_{y}\\hat{s}_{y}\\log\\sigma_{y}\\left(\\Phi \\beta\\right)\n",
    "\\end{align*}\n",
    "\n",
    "A common estimation method of $\\beta$ is by maximum likelihood%\n",
    "\n",
    "\\begin{align*}\n",
    "\\max_{\\beta}l\\left(  \\beta\\right)  .\n",
    "\\end{align*}\n",
    "\n",
    "MLE is statistically efficient; the problem is that the problem is not guaranteed to be convex, so there may be computational difficulties (e.g. local optima)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE, logit case\n",
    "\n",
    "In the logit case,\n",
    "\n",
    "\\begin{align*}\n",
    "l\\left(  \\beta\\right)  =N\\left\\{  \\hat{s}^{\\intercal}\\Phi\\beta-\\log\\sum_{y}\\exp\\left(  \\Phi\\beta\\right)  _{y}\\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "so that the max-likehood amounts to\n",
    "\n",
    "\\begin{align*}\n",
    "\\max_{\\beta}\\left\\{  \\hat{s}^{\\intercal} \\Phi \\beta-G\\left( \\Phi \\beta\\right)\n",
    "_{y}\\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "whose value is the Legendre-Fenchel transform of $\\beta\\rightarrow G\\left( \\Phi \\beta\\right)$ evaluated at $\\Phi ^{^{\\intercal}}\\hat{s}$.\n",
    "\n",
    "Note that the vector $\\Phi^{^{\\intercal}}\\hat{s}$ is the vector of empirical moments, which is a sufficient statistics in the logit model.\n",
    "\n",
    "As a result, in the logit case, the MLE is a convex optimization problem, and it is therefore both statistically efficient and computationally efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moment estimation\n",
    "\n",
    "The previous remark will inspire an alternative procedure based on the moments statistics $\\Phi^{^{\\intercal}}\\hat{s}$.\n",
    "\n",
    "The social welfare is given in general by $W\\left(  \\beta\\right) =G\\left(  \\Phi\\beta\\right)  $. One has $\\partial_{\\beta^{i}}W\\left(\\beta\\right)  =\\sum_{y}\\sigma_{y}\\left(  \\Phi\\beta\\right)  \\Phi_{yi}$, that is \n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla W\\left(  \\beta\\right)  = \\Phi^{\\intercal}\\sigma\\left(  \\Phi\\beta\\right)  ,\n",
    "\\end{align*}\n",
    "\n",
    "which is the vector of predicted moments.\n",
    "\n",
    "Therefore the program\n",
    "\n",
    "\\begin{align*}\n",
    "\\max_{\\beta}\\left\\{  \\hat{s}^{\\intercal}\\Phi\\beta-G\\left(  \\Phi\\beta\\right)\n",
    "_{y}\\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "picks up the parameter $\\beta$ which matches the empirical moments $X^{^{\\intercal}}\\hat{s}$ with the predicted ones $\\nabla W\\left(\\beta\\right)  $. This procedure is not statistically efficient, but is computationally efficient becauses it arises from a convex optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed temperature MLE\n",
    "\n",
    "Back to the logit case. Recall we have\n",
    "\n",
    "\\begin{align*}\n",
    "l\\left(  \\beta\\right)  =N\\left\\{  \\hat{s}^{\\intercal}\\Phi\\beta-\\log\\sum_{y} \\exp\\left(  \\Phi\\beta\\right)  _{y}\\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "Assume that we restrict ourselves to $\\beta^{\\top}z>0$. Then we can write $\\beta=\\theta/T$ where $T=1/\\beta^{\\top}z$ and $\\theta=\\beta T$. Call $\\Theta=\\left\\{  \\theta\\in\\mathbb{R}^{p},\\theta^{\\top}z=1\\right\\}  $, so that $\\beta=\\theta/T$ where $\\theta\\in\\Theta$ and $T>0$. We have\n",
    "\n",
    "\\begin{align*}\n",
    "l\\left(  \\theta,T\\right)  =\\frac{N}{T}\\left\\{  \\hat{s}^{\\intercal}\n",
    "\\Phi\\theta-T\\log\\sum_{y}\\exp\\left(  \\frac{\\left(  \\Phi\\theta\\right)  _{y}}{T}\\right)  \\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "and we define the *fixed temperature maximum likelihood estimator* by\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta\\left(  T\\right)  =\\arg\\max_{\\theta}l\\left(  \\theta,T\\right)\n",
    "\\end{align*}\n",
    "\n",
    " Note that $\\theta\\left(  T\\right)  =\\arg\\max_{\\theta\\in\\Theta}Tl\\left(\\theta,T\\right)$ where\n",
    "\n",
    "\\begin{align*}\n",
    "Tl\\left(  \\theta,T\\right)  =N\\left\\{  \\hat{s}^{\\intercal}\\Phi\\theta-T\\log\\sum _{y}\\exp\\left(  \\frac{\\left(  \\Phi\\theta\\right)  _{y}}{T}\\right)  \\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "and we note that $Tl\\left(  \\theta,T\\right)  \\rightarrow N\\left\\{  \\hat{s}^{\\intercal}\\Phi\\theta-\\max_{y\\in\\mathcal{Y}}\\left\\{  \\left(  \\Phi\\theta\\right)_{y}\\right\\}  \\right\\}  $ as $T\\rightarrow0$.\n",
    "\n",
    "We have\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{Tl\\left(  \\theta,T\\right)  }{N}=\\hat{s}^{\\intercal}\\Phi\\theta-T\\log\\sum_{y}\\exp\\left(  \\frac{\\left(  \\Phi\\theta\\right)  _{y}}{T}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Let $\\theta\\left(  0\\right)  =\\lim_{T\\rightarrow0}\\theta\\left(T\\right)  $. Calling $m\\left(  \\theta\\right)  =\\max_{y\\in\\mathcal{Y}}\\left\\{\\left(  \\Phi\\theta\\right)  _{y}\\right\\}  $, we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta\\left(  0\\right)  \\in\\arg\\max_{\\theta}\\left\\{  \\hat{s}^{\\intercal}\\Phi\\theta-m\\left(  \\theta\\right)  \\right\\},\n",
    "\\end{align*}\n",
    "\n",
    "or\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta\\left(  0\\right)  \\in\\arg\\min_{\\theta}\\left\\{  m\\left(  \\theta\\right)-\\hat{s}^{\\intercal}\\Phi\\theta\\right\\},\n",
    "\\end{align*}\n",
    "\n",
    "Calling $m\\left(  \\theta\\right)  =\\max_{y\\in\\mathcal{Y}}\\left\\{  \\left(\\Phi\\theta\\right)  _{y}\\right\\}  $, one has \n",
    "\n",
    "\\begin{align*}\n",
    "\\theta\\left(  T\\right)  \\in\\arg\\max\\left\\{  \\hat{s}^{\\intercal}\\Phi\\theta-m\\left(  \\theta\\right)  -T\\log\\sum_{y}\\exp\\left(  \\frac{\\left(\\Phi\\theta\\right)  _{y}-m\\left(  \\theta\\right)  }{T}\\right)  \\right\\}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimax-regret estimation\n",
    "\n",
    "Note that\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta\\left(  0\\right)  \\in\\arg\\max\\left\\{  \\hat{s}^{\\intercal}\\Phi\\theta\n",
    "-m\\left(  \\theta\\right)  \\right\\}  .\n",
    "\\end{align*}\n",
    "\n",
    "Define $R_{i}\\left(  \\theta,y\\right)  =\\left(  \\Phi\\theta\\right)_{y}-\\left(  \\Phi\\theta\\right)  _{y_{i}}$ the regret associated with observation $i$ with respect to $y$. This is equal to the difference between the payoff given by $y$ and the payoff obtained under observation $i$, denoting $y_{i}$ the action taken in observation $i$. The max-regret associated with observation $i$ is therefore\n",
    "\n",
    "\\begin{align*}\n",
    "\\max_{y\\in\\mathcal{Y}}R_{i}\\left(  \\theta,y\\right)  =\\max_{y\\in\\mathcal{Y}}\\left\\{  \\left(  \\Phi\\theta\\right)_{y}-\\left(  \\Phi\\theta\\right)_{y_{i}}\\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "and the max-regret associated with the sample is $\\frac{1}{N}\\sum\\max_{y\\in\\mathcal{Y}}\\left\\{  R_{i}\\left(  \\theta,y\\right)  \\right\\}  $, that is $\\max_{y\\in\\mathcal{Y}}\\left\\{  \\left(  \\Phi\\theta\\right)  _{y}\\right\\} - \\hat{s}^{\\intercal}X\\theta$.\n",
    "\n",
    "The minimax regret estimator\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\theta}^{MMR}=\\min_{\\theta}\\left\\{  m\\left(  \\theta\\right)  -\\hat\n",
    "{s}^{\\intercal}\\Phi\\theta\\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "which has a linear programming fomulation\n",
    "\n",
    "\\begin{align*}\n",
    "&  \\min_{m,\\theta}m-\\hat{s}^{\\intercal}\\Phi\\theta\\\\\n",
    "s.t.~ &  m-\\left(  \\Phi\\theta\\right)  _{y}\\geq\\forall y\\in\\mathcal{Y}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-identification\n",
    "\n",
    "Note that the set of $\\theta$ that enter the solution to the problem above is not unique, but is a convex set. Denoting $V$ the value of program, we can look for bounds of $\\theta^{\\intercal}d$ for a chosen direction $d$ by\n",
    "\n",
    "\\begin{align*}\n",
    "& \\min_{\\theta,m}/\\max_{\\theta,m}   \\theta^{\\intercal}d\\\\\n",
    "s.t.~  &  m-\\hat{s}^{\\intercal}X\\theta=V\\\\\n",
    "&  m\\geq\\left(  \\Phi\\theta\\right)_{y}, \\quad \\forall y\\in\\mathcal{Y}%\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link with exponential families and GLM\n",
    "\n",
    "See class notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation with observed heterogeneity\n",
    "\n",
    "We now assume that we observe individual characteristics that are relevant for individual choices, that is $U_{iy}=\\sum_k \\Phi_{iyk} \\beta_k$, or in matrix form\n",
    "$$U = \\Phi \\beta,$$ where $\\beta\\in\\mathbb{R}^{p}$ is a parameter, and $\\Phi$ is a $\\left(\\left\\vert \\mathcal{I}\\left\\vert\\right\\vert\\mathcal{Y}\\right\\vert \\right) \\times p$ matrix.\n",
    "\n",
    "See class notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application\n",
    "\n",
    "Back to the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi_iy_k = np.column_stack((np.kron(np.identity(4)[0:4,1:4],np.repeat(1, ninds).reshape(ninds,1)), - travelmode['travel'].values, - (travelmode['travel']*travelmode['income']).values, - travelmode['gcost'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbK = Phi_iy_k.shape[1]\n",
    "phi_mean = Phi_iy_k.mean(axis = 0)\n",
    "phi_stdev = Phi_iy_k.std(axis = 0, ddof = 1)\n",
    "Phi_iy_k = ((Phi_iy_k - phi_mean).T/phi_stdev[:,None]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(theta):\n",
    "    nbK = np.asarray(theta).shape[0]\n",
    "    Xtheta = Phi_iy_k.dot(theta)/sigma\n",
    "    Xthetamat_iy = Xtheta.reshape(nbchoices, ninds).T\n",
    "    max_i = np.amax(Xthetamat_iy, axis = 1)\n",
    "    expPhi_iy = np.exp((Xthetamat_iy.T -max_i).T)\n",
    "    d_i = np.sum(expPhi_iy, axis = 1)\n",
    "    \n",
    "    val = np.sum(np.multiply(Xtheta,muhat_iy))  - np.sum(max_i) - sigma * np.sum(np.log(d_i))\n",
    "\n",
    "    return -val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_log_likelihood(theta):\n",
    "    nbK = np.asarray(theta).shape[0]\n",
    "    Xtheta = Phi_iy_k.dot(theta)/sigma\n",
    "    Xthetamat_iy = Xtheta.reshape(nbchoices, ninds).T\n",
    "    max_i = np.amax(Xthetamat_iy, axis = 1)\n",
    "    expPhi_iy = np.exp((Xthetamat_iy.T -max_i).T)\n",
    "    d_i = np.sum(expPhi_iy, axis = 1)\n",
    "    \n",
    "    temp_mat = np.multiply(Phi_iy_k.T, expPhi_iy.T.flatten()).T\n",
    "    list_temp = []\n",
    "    for i in range(nbchoices):\n",
    "        list_temp.append(temp_mat[i*ninds:(i+1)*ninds,])\n",
    "    n_i_k = np.sum(list_temp,axis = 0)\n",
    "    \n",
    "    thegrad = muhat_iy.reshape(1,nbchoices*ninds).dot(Phi_iy_k).flatten() - np.sum(n_i_k.T/d_i, axis = 1)\n",
    "\n",
    "    return -thegrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0 = np.repeat(0,nbK)\n",
    "sigma = 1\n",
    "outcome = optimize.minimize(log_likelihood,method = 'CG',jac = grad_log_likelihood, x0 = theta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_mle = 1 / outcome['x'][nbK - 1]\n",
    "theta_mle = outcome['x']*temp_mle\n",
    "print(temp_mle)\n",
    "print(theta_mle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenobj = nbK+ninds\n",
    "c = np.concatenate((muhat_iy.reshape(1,nbchoices*ninds).dot(Phi_iy_k).flatten(),np.repeat(-1,ninds)))\n",
    "\n",
    "m = grb.Model('lp')\n",
    "x = m.addMVar(lenobj, name='x', lb=-grb.GRB.INFINITY)\n",
    "m.setObjective(c @ x, grb.GRB.MAXIMIZE)\n",
    "cstMat = spr.hstack((spr.csr_matrix(-Phi_iy_k), spr.kron(two_d(np.repeat(1,nbchoices)),spr.identity(ninds))))\n",
    "rhs = np.repeat(0,ninds*nbchoices)\n",
    "m.addConstr(cstMat @ x >= rhs)\n",
    "nbCstr = cstMat.shape[0]\n",
    "const_2 = np.array([0]*(nbK - 1))\n",
    "const_2 = np.append(const_2, 1)\n",
    "const_2 = np.append(const_2 ,[0]*ninds)\n",
    "m.addConstr(const_2 @ x == 1)\n",
    "m.optimize()\n",
    "if m.status == grb.GRB.Status.OPTIMAL:\n",
    "    print(\"Value of the problem (Gurobi) =\", m.objval)\n",
    "    opt_x = m.getAttr('x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_lp = np.array(opt_x[:nbK])\n",
    "print(theta_lp)\n",
    "print(theta_mle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indMax=100\n",
    "tempMax=temp_mle\n",
    "outcomemat = np.zeros((indMax+1,nbK-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_fixedtemp(subsetoftheta, *temp):\n",
    "    val = log_likelihood(np.append(subsetoftheta, 1/temp[0]))\n",
    "    \n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_log_likelihood_fixedtemp(subsetoftheta, *temp):\n",
    "    val = grad_log_likelihood(np.append(subsetoftheta, 1/temp[0]))[:-1]\n",
    "    \n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomemat[0,:] = theta_lp[:-1]\n",
    "iterMax = indMax+1\n",
    "for k in range(2,iterMax+1,1):\n",
    "    thetemp = tempMax * (k-1)/indMax\n",
    "    outcomeFixedTemp = optimize.minimize(log_likelihood_fixedtemp,method = 'CG',jac = grad_log_likelihood_fixedtemp, args = (thetemp,),  x0 = theta0[:-1])\n",
    "    outcomemat[k-1,:] = outcomeFixedTemp['x']*thetemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomemat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zero-temperature estimator is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outcomemat[1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mle estimator is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outcomemat[indMax,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbB = 100\n",
    "thetemp = 1\n",
    "epsilon_biy = special.digamma(1) -np.log(-np.log(np.random.uniform(0,1,ninds*nbchoices*nbB)))\n",
    "lenobj = ninds*nbB+nbK\n",
    "\n",
    "newc = np.concatenate((muhat_iy.reshape(1,nbchoices*ninds).dot(Phi_iy_k).flatten(),np.repeat(-1/nbB,ninds*nbB)))\n",
    "newm = grb.Model('new_lp')\n",
    "x = newm.addMVar(lenobj, name='x', lb=-grb.GRB.INFINITY)\n",
    "newm.setObjective(newc @ x, grb.GRB.MAXIMIZE)\n",
    "mat1 = spr.kron(-Phi_iy_k, two_d(np.repeat(1,nbB)))\n",
    "mat2 = spr.kron(two_d(np.repeat(1,nbchoices)),spr.identity(ninds*nbB))\n",
    "newcstMat = spr.hstack((mat1, mat2))\n",
    "rhs = epsilon_biy\n",
    "newm.addConstr(newcstMat @ x >= rhs)\n",
    "newm.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if m.status == grb.GRB.Status.OPTIMAL:\n",
    "    print(\"Value of the problem (Gurobi) =\", newm.objval)\n",
    "    opt_x = np.array(newm.getAttr('x'))\n",
    "newtheta_lp = opt_x[:nbK] / opt_x[nbK-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(theta_mle)\n",
    "print(newtheta_lp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally probit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbB = 100\n",
    "thetemp = 1\n",
    "epsilon_biy = np.random.normal(nbB*ninds*nbchoices)\n",
    "lenobj = ninds*nbB+nbK\n",
    "\n",
    "newc = np.concatenate((muhat_iy.reshape(1,nbchoices*ninds).dot(Phi_iy_k).flatten(),np.repeat(-1/nbB,ninds*nbB)))\n",
    "newm = grb.Model('new_lp')\n",
    "x = newm.addMVar(lenobj, name='x', lb=-grb.GRB.INFINITY)\n",
    "newm.setObjective(newc @ x, grb.GRB.MAXIMIZE)\n",
    "mat1 = spr.kron(-Phi_iy_k, two_d(np.repeat(1,nbB)))\n",
    "mat2 = spr.kron(two_d(np.repeat(1,nbchoices)),spr.identity(ninds*nbB))\n",
    "newcstMat = spr.hstack((mat1, mat2))\n",
    "rhs = epsilon_biy\n",
    "newm.addConstr(newcstMat @ x >= rhs)\n",
    "newm.optimize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
