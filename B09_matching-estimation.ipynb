{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Block 9: estimation of matching models</center>\n",
    "### <center>Alfred Galichon (NYU & Sciences Po)</center>\n",
    "## <center>'math+econ+code' masterclass on optimal transport and economic applications</center>\n",
    "<center>© 2018-2021 by Alfred Galichon. Past and present support from NSF grant DMS-1716489, ERC grant CoG-866274, and contributions by Jules Baudet, Pauline Corblet, Gregory Dannay, and James Nesbit are acknowledged.</center>\n",
    "\n",
    "#### <center>With python code examples</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "* Matching with unobserved heterogeneities\n",
    "\n",
    "* Estimation of matching models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "**[B]** Becker (1973). 'A Theory of Marriage: Part 1.' *Journal of Political Economy*.\n",
    "\n",
    "**[COQ]** Chiappori, Oreffice and Quintana-Domeque (2012). 'Fatter Attraction: Anthropometric and Socioeconomic Matching on the Marriage Market'. *Journal of Political Economy*.\n",
    "\n",
    "**[CS]** Choo and Siow (2006). 'Who Marries Whom and Why'. *Journal of Political Economy*.\n",
    "\n",
    "**[CSW]** Chiappori, Salanié, and Weiss (2017). 'Partner Choice and the Marital College Premium'. * American Economic Review*.\n",
    "\n",
    "**[DG]** Dupuy and Galichon (2014). 'Personality traits and the marriage market'. *Journal of Political Economy*.\n",
    "\n",
    "**[GS]** Galichon and Salanié (2020). 'Cupid's Invisible Hand: Social Surplus and Identification in Matching Models'. Preprint (first version 2011).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation: models of matching since Gary Becker\n",
    "\n",
    "* In the footsteps of Becker, empirical studies on the marriage market had long been focused on one-dimensional models, which assumes that a single index is enough to capture the interactions on the marriage market, and positive assortative matching (PAM), which predicts that the matching equilibrium will tend to match the agents with higher indices with each other.\n",
    "\n",
    "* However, it is desirable to move beyond PAM:\n",
    "    * PAM is always loosely true, never precisely\n",
    "    * there are often many observed characteristics, and it is not always the case that the sorting can be captured by a single-dimensional model\n",
    "    * PAM is a theoretical prediction stemming from assumptions of supermodularity of the surplus function which do not necessarly hold\n",
    "    * optimal transport provide tools to study multidimensional models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* However, any model of matching based on (unregularized) optimal transport will not be exploitable because it will generate far too strong predictions, namely that some matchings will never hold. This is rather counterfactual: in the data, one observes virtually any combination of type.\n",
    "\n",
    "* Hence, need to regularize the matching model, and we shall do so by introducing unobserved heterogeneity. The model so obtained will be exploitable for estimation and identification purposes. The first such model (with transfers) is the model by [CS]. We shall see a generalization of this model by [GS] (2015).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our libraries\n",
    "\n",
    "We start with loading the libraries we will need. They are rather standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as grb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A look at our data\n",
    "\n",
    "* Our data are Choo and Siow's original data. Choo and Siow wanted to study the impact of the legalization of abortion by the Roe vs. Wade decision by the supreme court on the 'value of marriage'. Roe vs. Wade decreased the role of marriage in covering out-of-the-wedlocks pregnancies ('shotgun weddings').\n",
    "\n",
    "* The decision did, however, not make a change uniformly in the United\n",
    "States as a number of states had already legalized abortion (reform states).\n",
    "Choo and Siow thus offer a diffs-in-diffs approach in order to compute the\n",
    "change in the value of marriage.\n",
    "\n",
    "* Choo and Siow's data are thus made of the marriages between men and women in reformed states (R) vs nonreformed states (NR), in 1972 and in 1982. One should expect to see a higher drop in marriage value in NR states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "thepath = os.getcwd()\n",
    "n_singles = pd.read_csv(os.path.join(thepath,'data_mec_optim/marriage-ChooSiow/n_singles.txt'), sep='\\t', header = None)\n",
    "marr = pd.read_csv(os.path.join(thepath,'data_mec_optim/marriage-ChooSiow/marr.txt'), sep='\\t', header = None)\n",
    "navail = pd.read_csv(os.path.join(thepath,'data_mec_optim/marriage-ChooSiow/n_avail.txt'), sep='\\t', header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used by Choo and Siow is census data on marriages between age categories, from age 16 (row/column 0) to age 75 (row/age 59). It is thus 60x60 tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22704</td>\n",
       "      <td>10954</td>\n",
       "      <td>3932</td>\n",
       "      <td>1550</td>\n",
       "      <td>672</td>\n",
       "      <td>414</td>\n",
       "      <td>190</td>\n",
       "      <td>114</td>\n",
       "      <td>78</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40266</td>\n",
       "      <td>38980</td>\n",
       "      <td>16368</td>\n",
       "      <td>5714</td>\n",
       "      <td>2116</td>\n",
       "      <td>1101</td>\n",
       "      <td>691</td>\n",
       "      <td>427</td>\n",
       "      <td>260</td>\n",
       "      <td>127</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39219</td>\n",
       "      <td>49753</td>\n",
       "      <td>40315</td>\n",
       "      <td>18001</td>\n",
       "      <td>5401</td>\n",
       "      <td>2291</td>\n",
       "      <td>1429</td>\n",
       "      <td>856</td>\n",
       "      <td>611</td>\n",
       "      <td>338</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29500</td>\n",
       "      <td>44788</td>\n",
       "      <td>47212</td>\n",
       "      <td>39429</td>\n",
       "      <td>16730</td>\n",
       "      <td>6270</td>\n",
       "      <td>3211</td>\n",
       "      <td>1910</td>\n",
       "      <td>762</td>\n",
       "      <td>438</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18952</td>\n",
       "      <td>32860</td>\n",
       "      <td>40103</td>\n",
       "      <td>43488</td>\n",
       "      <td>36265</td>\n",
       "      <td>15318</td>\n",
       "      <td>6076</td>\n",
       "      <td>2872</td>\n",
       "      <td>1552</td>\n",
       "      <td>1246</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1      2      3      4      5     6     7     8     9   ...  50  \\\n",
       "0  22704  10954   3932   1550    672    414   190   114    78    64  ...   0   \n",
       "1  40266  38980  16368   5714   2116   1101   691   427   260   127  ...   0   \n",
       "2  39219  49753  40315  18001   5401   2291  1429   856   611   338  ...   0   \n",
       "3  29500  44788  47212  39429  16730   6270  3211  1910   762   438  ...   0   \n",
       "4  18952  32860  40103  43488  36265  15318  6076  2872  1552  1246  ...   0   \n",
       "\n",
       "   51  52  53  54  55  56  57  58  59  \n",
       "0   0   0   0   0   0   0   0   0   0  \n",
       "1   0   0   0   0   0   0   0   0   0  \n",
       "2   0   0   0   0   0   0   0   0   0  \n",
       "3   0   0   0   0   0   0   0   0   0  \n",
       "4   0   0   0   0   0   0   0   0   0  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data also includes the number of single individuals per age category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1010132</td>\n",
       "      <td>907226</td>\n",
       "      <td>772448</td>\n",
       "      <td>597919</td>\n",
       "      <td>454792</td>\n",
       "      <td>341370</td>\n",
       "      <td>315797</td>\n",
       "      <td>253618</td>\n",
       "      <td>171614</td>\n",
       "      <td>152228</td>\n",
       "      <td>...</td>\n",
       "      <td>68383</td>\n",
       "      <td>69177</td>\n",
       "      <td>61358</td>\n",
       "      <td>70752</td>\n",
       "      <td>69112</td>\n",
       "      <td>60909</td>\n",
       "      <td>60749</td>\n",
       "      <td>65302</td>\n",
       "      <td>62922</td>\n",
       "      <td>61117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>790793</td>\n",
       "      <td>671332</td>\n",
       "      <td>602900</td>\n",
       "      <td>492020</td>\n",
       "      <td>404882</td>\n",
       "      <td>313755</td>\n",
       "      <td>255414</td>\n",
       "      <td>195418</td>\n",
       "      <td>141283</td>\n",
       "      <td>128787</td>\n",
       "      <td>...</td>\n",
       "      <td>197456</td>\n",
       "      <td>201183</td>\n",
       "      <td>191519</td>\n",
       "      <td>218031</td>\n",
       "      <td>219157</td>\n",
       "      <td>199926</td>\n",
       "      <td>200052</td>\n",
       "      <td>202146</td>\n",
       "      <td>210315</td>\n",
       "      <td>202775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0       1       2       3       4       5       6       7       8   \\\n",
       "0  1010132  907226  772448  597919  454792  341370  315797  253618  171614   \n",
       "1   790793  671332  602900  492020  404882  313755  255414  195418  141283   \n",
       "\n",
       "       9   ...      50      51      52      53      54      55      56  \\\n",
       "0  152228  ...   68383   69177   61358   70752   69112   60909   60749   \n",
       "1  128787  ...  197456  201183  191519  218031  219157  199926  200052   \n",
       "\n",
       "       57      58      59  \n",
       "0   65302   62922   61117  \n",
       "1  202146  210315  202775  \n",
       "\n",
       "[2 rows x 60 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_singles.transpose().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model\n",
    "\n",
    "The analysis here follows [GS], who build on the logit model by [CS]. \n",
    "\n",
    "* Consider a heterosexual marriage matching market. The set of types (observable characteristics) is $\\mathcal{X}$ for men, and $\\mathcal{Y}$ for women. There are $n_{x}$ men of type $x$, and $m_{y}$ women of type $y$.\n",
    "\n",
    "* Assume that if a man $i\\in\\mathcal{I}$ of type $x_{i}$ and a woman $j\\in\\mathcal{J}$ of type $y_{j}$ match, they get respective utilities \\begin{align*} &  \\alpha_{x_{i}y_{j}}+w_{ij}+\\varepsilon_{iy_{j}}\\\\ &  \\gamma_{x_{i}y_{j}}-w_{ij}+\\eta_{x_{i}j} \\end{align*} where $w_{ij}$ is the transfer from $i$ to $j$. If they remain single $i$ and $j$ get respectively $\\varepsilon_{i0}$ and $\\eta_{0j}$.\n",
    "\n",
    "* The random utility vectors $\\left(  \\varepsilon_{y}\\right)  $ and $\\left(  \\eta_{x}\\right)  $ are drawn from probability distributions $\\mathbf{P}_{x}$ and $\\mathbf{Q}_{y}$, respectively. In the sequel we shall work with a finite number of agents of each type, and then we'll investigate the limit of these results.\n",
    "\n",
    "* The fact that the preferences for the other side of the market terms $\\varepsilon_{iy_{j}}$ and $\\eta_{x_{i}}$ do not vary within observed types is a very important implicit assumption called **separability**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal matching\n",
    "\n",
    "\n",
    "The matching surplus between $i$ and $j$ is therefore $$\\tilde{\\Phi}_{ij}=\\Phi_{x_{i}y_{j}}+\\varepsilon_{iy_{j}}+\\eta_{x_{i}j}$$ where $\\Phi_{xy}=\\alpha_{xy}+\\gamma_{xy}$. The value of optimal matching is thus, under its dual form, \\begin{align*}\\min_{u_{i},v_{j}}  &  \\sum_{i\\in\\mathcal{I}}u_{i}+\\sum_{j\\in\\mathcal{J}}  v_{j}\\\\ s.t.~  &  u_{i}+v_{j}\\geq\\Phi_{x_{i}y_{j}}+\\varepsilon_{iy_{j}}+\\eta_{x_{i}j}\\\\ &  u_{i}\\geq\\varepsilon_{i0}\\\\ &  v_{j}\\geq\\eta_{j0} \\end{align*}\n",
    "\n",
    "Written like this, the lp has $\\left\\vert \\mathcal{I}\\right\\vert +\\left\\vert \\mathcal{J}\\right\\vert $ variables and $\\left\\vert \\mathcal{I} \\right\\vert \\times\\left\\vert \\mathcal{J}\\right\\vert +\\left\\vert \\mathcal{I} \\right\\vert +\\left\\vert \\mathcal{J}\\right\\vert $ constraints. Assuming that there are $K$ individuals per type for each type, this is $K\\left(  \\left\\vert \\mathcal{X}\\right\\vert +\\left\\vert \\mathcal{Y}\\right\\vert \\right)  $ variables and $K^{2}\\left(  \\left\\vert \\mathcal{X}\\right\\vert \\times\\left\\vert \\mathcal{Y}\\right\\vert \\right)  +K\\left(  \\left\\vert \\mathcal{X}\\right\\vert +\\left\\vert \\mathcal{Y}\\right\\vert \\right)  $ constraints.\n",
    "\n",
    "The number of constraints is **quadratic** with respect to $K$. Fortunately, a little thinking about the implications of separability will help us reduce this complexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A property of equilibrium\n",
    "\n",
    "We have:\n",
    "\n",
    "---\n",
    "\n",
    "**Lemma**. Consider the set $\\mathcal{I}_{xy}$ of men of type $x$ matched to women of type $y$ at equilibrium. If $\\mathcal{I}_{xy}$ is nonempty, then $u_{i}-\\varepsilon_{iy}$ is a constant across $\\mathcal{I}_{xy}$.\n",
    "\n",
    "---\n",
    "\n",
    "**Proof**. For $i\\in\\mathcal{I}$ such that $x_{i}=x$,\n",
    "\\begin{align*}\n",
    "u_{i}  &  =\\max_{j\\in\\mathcal{J}}\\left\\{  \\tilde{\\Phi}_{ij}-v_{j}%\n",
    ",\\varepsilon_{i0}\\right\\} \\\\\n",
    "&  =\\max_{y\\in\\mathcal{Y}}\\left\\{  U_{xy}+\\varepsilon_{iy},\\varepsilon\n",
    "_{i0}\\right\\}\n",
    "\\end{align*}\n",
    "where $U_{xy}=\\max_{j:y_{j}=y}\\left\\{  \\Phi_{xy}+\\eta_{x_{i}j}-v_{j}\\right\\}\n",
    "$, thus $u_{i}\\geq U_{xy}+\\varepsilon_{iy}$ with equality on $\\mathcal{I}%\n",
    "_{xy}$. With similar notations, $v_{j}\\geq V_{xy}+\\eta_{xj}$ with equality on\n",
    "$\\mathcal{J}_{xy}$. As a result, if $\\mathcal{I}_{xy}$ is nonempty, then\n",
    "$U_{xy}+V_{xy}=\\Phi_{xy}$ and $\\forall i\\in\\mathcal{I}_{xy},$ $u_{i}%\n",
    "=U_{xy}+\\varepsilon_{iy}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simplification\n",
    "\n",
    "In the sequel, we shall see that *adding* an auxiliary variable to\n",
    "the previous lp will lead to *decreasing* the computational complexity of\n",
    "the problem.\n",
    "\n",
    "Observe that the first set of constraints is reexpressed by saying that,\n",
    "for every $x\\in\\mathcal{X}$, $y\\in\\mathcal{Y}$,\n",
    "$$\n",
    "\\min_{i:x_{i}=x}\\left\\{  u_{i}-\\varepsilon_{iy}\\right\\}  +\\min_{j:y_{j}\n",
    "=y}\\left\\{  v_{j}-\\eta_{xj}\\right\\}  \\geq\\Phi_{xy}.\n",
    "$$\n",
    "\n",
    "\n",
    "Hence, letting $U_{xy}=\\min_{i:x_{i}=x}\\left\\{  u_{i}-\\varepsilon\n",
    "_{iy}\\right\\}  $ and $V_{xy}=\\min_{j:y_{j}=y}\\left\\{  v_{j}-\\eta_{xj}\\right\\}\n",
    "$, a solution of the previous lp should satisfy\n",
    "$$\n",
    "u_{i}=\\max_{y\\in\\mathcal{Y}}\\left\\{  U_{xy}+\\varepsilon_{iy},\\varepsilon\n",
    "_{i0}\\right\\}  \\text{ and }v_{j}=\\max_{x\\in\\mathcal{X}}\\left\\{  V_{xy}\n",
    "+\\varepsilon_{xj},\\varepsilon_{0j}\\right\\}  .\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem rewrites as\n",
    "\\begin{align}\n",
    "\\min_{u_{i},v_{j},U_{xy},V_{xy}}  &  \\sum_{i\\in\\mathcal{I}}u_{i}+\\sum\n",
    "_{j\\in\\mathcal{J}}v_{j}\\label{simplifiedDual}\\\\\n",
    "s.t.~  &  U_{xy}+V_{xy}\\geq\\Phi_{xy}~\\left[  \\mu_{xy}\\geq0\\right] \\nonumber\\\\\n",
    "&  u_{i}\\geq U_{x_{i}y}+\\varepsilon_{iy_{j}}~\\left[  \\mu_{iy}\\right]\n",
    "\\nonumber\\\\\n",
    "&  v_{j}\\geq V_{xy_{j}}+\\eta_{x_{i}j}~\\left[  \\mu_{xi}\\right] \\nonumber\\\\\n",
    "&  u_{i}\\geq\\varepsilon_{i0}~\\left[  \\mu_{i0}\\right] \\nonumber\\\\\n",
    "&  v_{j}\\geq\\eta_{j0}~\\left[  \\mu_{0x}\\right] \\nonumber\n",
    "\\end{align}\n",
    "\n",
    "This problem has $K\\left(  \\left\\vert \\mathcal{X}\\right\\vert +\\left\\vert\n",
    "\\mathcal{Y}\\right\\vert \\right)  +\\left\\vert \\mathcal{X}\\right\\vert\n",
    "\\times\\left\\vert \\mathcal{Y}\\right\\vert $ variables and $\\left(  \\left\\vert\n",
    "\\mathcal{X}\\right\\vert \\times\\left\\vert \\mathcal{Y}\\right\\vert \\right)\n",
    "+K\\left(  2\\left\\vert \\mathcal{X}\\right\\vert \\times\\left\\vert \\mathcal{Y}\n",
    "\\right\\vert +\\left\\vert \\mathcal{X}\\right\\vert +\\left\\vert \\mathcal{Y}\n",
    "\\right\\vert \\right)  $ constraints.\n",
    "\n",
    "The number of constraint is now **linear** with respect to $K$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consequences\n",
    "\n",
    "**1. Lagrange multipliers:**\n",
    "* The Lagrange multiplier $\\mu_{xy}$ is interpreted as the number of matchings between types $x$ and $y$.\n",
    "\n",
    "* The Lagrange multiplier $\\mu_{iy}$ ($y\\in\\mathcal{Y}_{0}$) is interpreted as a 0-1 indicator that man $i$ chooses a type $y$\n",
    "\n",
    "* The Lagrange multiplier $\\mu_{xj}$ ($x\\in\\mathcal{X}_{0}$) is interpreted as a 0-1 indicator that woman $j$ chooses a\\ type $x$\n",
    "\n",
    "**2. Utilities:**\n",
    "* Man $i$ solves a discrete choice problem $u_{i}=\\max_{y\\in\\mathcal{Y}\n",
    "}\\left\\{  U_{xy}+\\varepsilon_{iy},\\varepsilon_{i0}\\right\\}  $\n",
    "\n",
    "* Woman $j$ solve a discrete choice problem $v_{j}=\\max_{x\\in\\mathcal{X}\n",
    "}\\left\\{  V_{xy}+\\eta_{xj},\\eta_{0j}\\right\\}  .$\n",
    "\\end{itemize}\n",
    "\n",
    "* $U_{xy}$ and $V_{xy}$ are related by $U_{xy}+V_{xy}\\geq\\Phi_{xy}$ with\n",
    "equality if $\\mu_{xy}>0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large market limit\n",
    "\n",
    "Now look at the limit of previous markets when the number of market participants gets large, holding fixed the frequency of each types.\n",
    "\n",
    "In the large population limit $n_{x}$ and $m_{y}$ are now interpreted as the mass distribution of respective types $x$ and $y$.\n",
    "\n",
    "We shall from now on assume that $\\mathbf{P}_{x}$ and $\\mathbf{Q}_{y}$, the distributions of random utility vectors $\\left( \\varepsilon_{y}\\right)  $ and $\\left(  \\eta_{x}\\right)  $, have a density with full support. This will ensure that the Emax operators associated with the choice problems of the men and the women respectively \\begin{align*} & G_x(U_{x.}) = \\mathbb{E}_\\mathbf{P} \\left[\\max_{y\\in\\mathcal{Y}\n",
    "}\\left\\{  U_{xy}+\\varepsilon_{iy},\\varepsilon_{i0}\\right\\} \\right]\\text{, and }\\\\ & H_y(V_{.y}) = \\mathbb{E}_\\mathbf{Q} \\left[\\max_{x\\in\\mathcal{X}\n",
    "}\\left\\{  V_{xy}+\\eta_{xj},\\eta_{0j}\\right\\} \\right],\\end{align*}as well as the corresponding entropies of choice $G_x^{\\ast}$ and $H_y^{\\ast}$ are continuously differentiable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under these assumptions, the problem becomes\n",
    "\\begin{align*}\n",
    "\\min_{U,V} ~&  G\\left(  U\\right)  +H\\left(  V\\right) \\\\\n",
    "s.t.~  &  U_{xy}+V_{xy}\\geq\\Phi_{xy}~\\left[  \\mu_{xy}\\right]\n",
    "\\end{align*}\n",
    "where\n",
    "\\begin{align*}\n",
    "G\\left(  U\\right)   &  =\\sum_{x\\in\\mathcal{X}}n_{x}\\mathbb{E}_{\\mathbf{P}%\n",
    "}\\left[  \\max_{y\\in\\mathcal{Y}}\\left\\{  U_{xy}+\\varepsilon_{iy},\\varepsilon\n",
    "_{i0}\\right\\}  \\right] \\\\\n",
    "H\\left(  V\\right)   &  =\\sum_{y\\in\\mathcal{Y}}m_{y}\\mathbb{E}_{\\mathbf{Q}%\n",
    "}\\left[  \\max_{x\\in\\mathcal{X}}\\left\\{  V_{xy}+\\eta_{xj},\\eta_{0j}\\right\\}\n",
    "\\right]\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "By first order conditions,\n",
    "$$\n",
    "\\frac{\\partial G\\left(  U\\right)  }{\\partial U_{xy}}=\\mu_{xy}=\\frac{\\partial\n",
    "H\\left(  V\\right)  }{\\partial V_{xy}}.\n",
    "$$\n",
    "and $\\mu_{xy}>0$ for every $x\\in\\mathcal{X}$ and $y\\in\\mathcal{Y}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social planner's problem\n",
    "\n",
    "The primal problem corresponding the problem above is\n",
    "$$\n",
    "\\max_{\\mu_{xy}\\geq0}\\sum_{\\substack{x\\in\\mathcal{X}\\\\y\\in\\mathcal{Y}}}\\mu\n",
    "_{xy}\\Phi_{xy}-\\mathcal{E}\\left(  \\mu\\right)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\mathcal{E}\\left(  \\mu\\right)  =G^{\\ast}\\left(  \\mu\\right)  +H^{\\ast}\\left(\n",
    "\\mu\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "Recall $G^{\\ast}\\left(  \\mu\\right)  =\\max\\left\\{  \\sum_{xy}\\mu\n",
    "_{xy}U_{xy}-G\\left(  U\\right)  \\right\\}  $ is the Legendre transform of $G$, and similarly for $H^{\\ast}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identification of the matching surplus\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Theorem.** By first order conditions, we get the identifcation formula of $\\Phi$%\n",
    "$$\n",
    "\\Phi_{xy}=\\frac{\\partial G^{\\ast}\\left(  \\mu\\right)  }{\\partial\\mu_{xy}}\n",
    "+\\frac{\\partial H^{\\ast}\\left(  \\mu\\right)  }{\\partial\\mu_{xy}}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "This means that the surplus function is identified *nonparametrically* given the matching patterns $\\mu$ and assuming a fixed distribution of unobserved heterogeneity.\n",
    "\n",
    "Hence only the joint surplus $\\Phi\n",
    "_{xy}=\\alpha_{xy}+\\gamma_{xy}$ is identified. However, if the transfers\n",
    "$\\hat{w}_{xy}$ are observed too (e.g. wages in labour market), then\n",
    "$U_{xy}=\\alpha_{xy}+w_{xy}$ and $V_{xy}=\\gamma_{xy}-w_{xy}$, so that $\\alpha$\n",
    "and $\\gamma$ are separately identified by\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{array}\n",
    "[c]{c}%\n",
    "\\hat{\\alpha}_{xy}=\\frac{\\partial G^{\\ast}\\left(  \\mu\\right)  }{\\partial\\mu_{xy}}-\\hat{w}_{xy}\\\\\n",
    "\\hat{\\gamma}_{xy}=\\frac{\\partial H^{\\ast}\\left(  \\mu\\right)  }{\\partial\\mu_{xy}}+\\hat{w}_{xy}%\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choo and Siow's logit model\n",
    "\n",
    "In Choo and Siow's model [CS], the heterogeneities in tastes are Gubmel,\n",
    "we have\n",
    "$$\n",
    "\\mathcal{E}\\left(  \\mu\\right)  =2\\sum_{\\substack{x\\in\\mathcal{X}%\n",
    "\\\\y\\in\\mathcal{Y}}}\\mu_{xy}\\log\\mu_{xy}+\\sum_{x\\in\\mathcal{X}}\\mu_{x0}\\log\n",
    "\\mu_{x0}+\\sum_{y\\in\\mathcal{Y}}\\mu_{0y}\\log\\mu_{0y}.\n",
    "$$\n",
    "Note that $\\mathcal{E}\\left(  \\mu\\right)  < + \\infty$ if and only if $\\mu\n",
    "\\in\\mathcal{M}\\left(  n,m\\right)  $.\n",
    "\n",
    "\n",
    "By first order conditions above, Choo-Siow's TU-logit model implies the\n",
    "following matching function:\n",
    "$$\n",
    "\\mu_{xy}=M_{xy}\\left(  \\mu_{x0},\\mu_{0y}\\right)  :=\\sqrt{\\mu_{x0}}\\sqrt\n",
    "{\\mu_{0y}}\\exp\\left(  \\frac{\\Phi_{xy}}{2}\\right) \n",
    "$$\n",
    "\n",
    "This is a gravity equation of sorts. The full link with gravity equations is explored in the next lecture. \n",
    "\n",
    "As a result, $\\partial\\mathcal{E}\\left(  \\mu\\right)  /\\partial\\mu\n",
    "_{xy}=2\\log\\mu_{xy}-\\log\\mu_{x0}-\\log\\mu_{0y}$, which implies that $\\Phi_{xy}$\n",
    "is estimated by *Choo and Siow's identification formula*\n",
    "$$\n",
    "\\hat{\\Phi}_{xy}=\\log\\frac{\\hat{\\mu}_{xy}^{2}}{\\hat{\\mu}_{x0}\\hat{\\mu}_{0y}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving equilibrium in the Choo-Siow model\n",
    "\n",
    "Write down the equilibrium equations in the TU-logit model:\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{array}\n",
    "[c]{c}\n",
    "\\sum_{y\\in\\mathcal{Y}}\\sqrt{\\mu_{x0}}\\sqrt{\\mu_{0y}}\\exp\\left(  \\frac\n",
    "{\\Phi_{xy}}{2}\\right)  +\\mu_{x0}=n_{x}\\\\\n",
    "\\sum_{x\\in\\mathcal{X}}\\sqrt{\\mu_{x0}}\\sqrt{\\mu_{0y}}\\exp\\left(  \\frac\n",
    "{\\Phi_{xy}}{2}\\right)  +\\mu_{0y}=m_{y}%\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "\n",
    "Setting $a_{x}=\\sqrt{\\mu_{x0}}$, $b_{y}=\\sqrt{\\mu_{0y}}$, and\n",
    "$K_{xy}=\\exp\\left(  \\Phi_{xy}/2\\right)  $, this rewrites as\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{array}\n",
    "[c]{c}\n",
    "\\sum_{y\\in\\mathcal{Y}}K_{xy}a_{x}b_{y}+a_{x}^{2}=n_{x}\\\\\n",
    "\\sum_{x\\in\\mathcal{X}}K_{xy}a_{x}b_{y}+b_{y}^{2}=m_{y}%\n",
    "\\end{array}\n",
    "\\right.$$\n",
    "\n",
    "which is a variant of the equations previously seen to accomodate unmatched agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily adapt the IPFP to this setting. The IPFP will consists in iteratively solving quadratic equations:\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{array}\n",
    "[c]{l}\n",
    "a_{x}^{2t+1}=\\sqrt{n_{x}+\\left(  \\sum_{y\\in\\mathcal{Y}}b_{y}^{2t}%\n",
    "K_{xy}/2\\right)  ^{2}}-\\sum_{y\\in\\mathcal{Y}}b_{y}^{2t}K_{xy}/2\\\\\n",
    "b_{y}^{2t+2}=\\sqrt{m_{y}+\\left(  \\sum_{x\\in\\mathcal{X}}a_{x}^{2t+1}%\n",
    "K_{xy}/2\\right)  ^{2}}-\\sum_{x\\in\\mathcal{X}}a_{x}^{2t+1}K_{xy}/2\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dual problem\n",
    "\n",
    "The dual problem is given by\n",
    "$$\n",
    "\\min_{u,v}\\left\\{\n",
    "\\begin{array}\n",
    "[c]{c}%\n",
    "\\sum_{x}n_{x}u_{x}+\\sum_{y}m_{y}v_{y}\\\\\n",
    "+2\\sum_{xy}\\sqrt{n_{x}m_{y}}\\exp\\left(  \\frac{\\Phi_{xy}-u_{x}-v_{y}}{2}\\right)\n",
    "\\\\\n",
    "+\\sum_{x}n_{x}\\exp\\left(  -u_{x}\\right)  +\\sum_{y}m_{y}\\exp\\left(\n",
    "-v_{y}\\right)\n",
    "\\end{array}\n",
    "\\right\\}\n",
    "$$\n",
    "\n",
    "\n",
    "**Remarks.**\n",
    "\n",
    "* This problem is an unconstrained convex optimization problem, so this formulation will be quite useful.\n",
    "\n",
    "* If $\\left(  u,v\\right)  $ is solution, $u_{x}=-\\log\\mu_{0|x}=-\\log\\left(  \\mu_{x0}/n_{x}\\right)  $ and $v_{y}=-\\log\\left(  \\mu\n",
    "_{0|y}\\right)  $.\n",
    "\n",
    "* Note that the IPFP algorithm just seen interprets as (blockwise) *coordinate descent* method in the dual problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another application: estimation of affinity matrix\n",
    "\n",
    "Dupuy and G (2014) focus on cross-dimensional interactions\n",
    "\n",
    "\\begin{align*}\n",
    "\\phi_{xy}^{A}=\\sum_{p,q}A_{pq}\\xi_{x}^{p}\\xi_{y}^{q}\n",
    "\\end{align*}\n",
    "\n",
    "and estimate \"affinity matrix\" $A$ on a dataset of married individuals where the \"big 5\" personality traits are measured.\n",
    "\n",
    "$A$ is estimated by\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{s_{i},m_{n}}\\min_{A}\\left\\{\n",
    "\\begin{array}\n",
    "[c]{c}%\n",
    "\\sum_{x}p_{x}u_{x}+\\sum_{y}q_{y}v_{y}\\\\\n",
    "+\\sum_{xy}\\exp\\left(  \\sum_{p,q}A_{pq}\\xi_{x}^{p}\\xi_{y}^{q}-u_{x}%\n",
    "-v_{y}\\right) \\\\\n",
    "-\\sum_{x,y,p,q}\\hat{\\pi}_{xy}A_{pq}\\xi_{x}^{p}\\xi_{y}^{q}%\n",
    "\\end{array}\n",
    "\\right\\}  .\n",
    "\\end{align*}\n",
    "\n",
    "Dupuy, Galichon and Sun (2016) consider the case when the space of characteristics is high-dimensional. More on this soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation of affinity matrix: results\n",
    "\n",
    "|  Husbands   \\ Wives             | Education | Height | BMI   | Health | Consc. | Extra. | Agree | Emotio | Auto. | Risk  |\n",
    "|-------------------|-----------|--------|-------|--------|--------|--------|-------|--------|-------|-------|\n",
    "| Education         | 0.46      | 0      | -0.06 | 0.01   | -0.02  | 0.03   | -0.01 | -0.03  | 0.04  | 0.01  |\n",
    "| Height            | 0.04      | 0.21   | 0.04  | 0.03   | -0.06  | 0.03   | 0.02  | 0      | -0.01 | 0.02  |\n",
    "| BMI               | -0.03     | 0.03   | 0.21  | 0.01   | 0.03   | 0      | -0.05 | 0.02   | 0.01  | -0.02 |\n",
    "| Health            | -0.02     | 0.02   | -0.04 | 0.17   | -0.04  | 0.02   | -0.01 | 0.01   | 0     | 0.03  |\n",
    "| Conscienciousness | -0.07     | -0.01  | 0.07  | 0      | 0.16   | 0.05   | 0.04  | 0.06   | 0.01  | 0.01  |\n",
    "| Extraversion      | 0         | -0.01  | 0     | 0.01   | -0.06  | 0.08   | -0.04 | -0.01  | 0.02  | -0.06 |\n",
    "| Agreeableness     | 0.01      | 0.01   | -0.06 | 0.02   | 0.1    | -0.11  | 0     | 0.07   | -0.07 | -0.05 |\n",
    "| Emotional         | 0.03      | -0.01  | 0.04  | 0.06   | 0.19   | 0.04   | 0.01  | -0.04  | 0.08  | 0.05  |\n",
    "| Autonomy          | 0.03      | 0.02   | 0.01  | 0.02   | -0.09  | 0.09   | -0.04 | 0.02   | -0.1  | 0.03  |\n",
    "| Risk              | 0.03      | -0.01  | -0.03 | -0.01  | 0      | -0.02  | -0.03 | -0.03  | 0.08  | 0.14  |\n",
    "\n",
    "Affinity matrix. Source: Dupuy and G (2014). Note: Bold coefficients are significant at the 5 percent level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choo-Siow application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbCateg = 25\n",
    "\n",
    "muhat_x0 = n_singles[0].iloc[0:nbCateg]\n",
    "muhat_0y = n_singles[1].iloc[0:nbCateg]\n",
    "muhat_xy = marr.iloc[0:nbCateg:,0:nbCateg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nh = muhat_xy.values.sum()+muhat_x0.sum()+muhat_0y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14885023"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*muhat_xy.values.sum()+muhat_x0.sum()+muhat_0y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "muhat_xy = muhat_xy / Nh \n",
    "muhat_x0 = muhat_x0 / Nh \n",
    "muhat_0y = muhat_0y / Nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = muhat_xy.sum(axis = 1)+muhat_x0\n",
    "m_y = muhat_xy.sum(axis = 0)+muhat_0y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbX = nbCateg\n",
    "nbY = nbCateg\n",
    "\n",
    "xs = np.repeat(range(1,nbX+1),nbY).reshape(nbX,nbY)/25\n",
    "ys = np.repeat(range(1,nbY+1),nbX).reshape(nbX,nbY).T/25\n",
    "\n",
    "phi1_xy = -((xs-ys)**2).flatten()\n",
    "phimat = np.column_stack((phi1_xy,np.multiply(phi1_xy,(((xs+ys)/2)**2).flatten()),np.multiply(phi1_xy,(((xs+ys-2)/2)**2).flatten()),np.multiply(phi1_xy,((xs+ys-1)**2).flatten())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbK = phimat.shape[1]\n",
    "phimat_mean = phimat.mean(axis = 0)\n",
    "phimat_stdev = phimat.std(axis = 0, ddof = 1)\n",
    "phimat = ((phimat - phimat_mean).T/phimat_stdev[:,None]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ObjFunc(uvlambda):\n",
    "    u_x = uvlambda[0:nbX]\n",
    "    v_y = uvlambda[nbX:(nbX+nbY)]\n",
    "    l = uvlambda[(nbX+nbY):(nbX+nbY+nbK)]\n",
    "    \n",
    "    Phi_xy = phimat.dot(l.reshape(nbK,1)).reshape(nbX, nbY)\n",
    "    mu_xy = np.exp(((Phi_xy - u_x).T-v_y).T/2)\n",
    "    mu_x0 = np.exp(-u_x)\n",
    "    mu_0y = np.exp(-v_y)\n",
    "    \n",
    "    val = sum(np.multiply(n_x,u_x))+sum(np.multiply(m_y,v_y))-np.sum(np.multiply(muhat_xy.values,Phi_xy), axis = (0,1)) + 2*np.sum(mu_xy, axis =(0,1)) + sum(mu_x0) + sum(mu_0y)\n",
    "    \n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_ObjFunc(uvlambda):\n",
    "    u_x = uvlambda[0:nbX]\n",
    "    v_y = uvlambda[nbX:(nbX+nbY)]\n",
    "    l = uvlambda[(nbX+nbY):(nbX+nbY+nbK)]\n",
    "    \n",
    "    Phi_xy = phimat.dot(l.reshape(nbK,1)).reshape(nbX, nbY)\n",
    "    mu_xy = np.exp(((Phi_xy - u_x).T-v_y).T/2)\n",
    "    mu_x0 = np.exp(-u_x)\n",
    "    mu_0y = np.exp(-v_y)\n",
    "    \n",
    "    grad_u = n_x - np.sum(mu_xy, axis = 0) - mu_x0\n",
    "    grad_v = m_y - np.sum(mu_xy, axis = 1) - mu_0y\n",
    "    grad_lambda = (mu_xy-muhat_xy.values).flatten()[:,None].T.dot(phimat)\n",
    "    \n",
    "    grad = np.concatenate((grad_u,grad_v,grad_lambda.flatten()))\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n",
       "        0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n",
       "        0.04, 0.04, 0.04],\n",
       "       [0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08,\n",
       "        0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08,\n",
       "        0.08, 0.08, 0.08],\n",
       "       [0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12,\n",
       "        0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12,\n",
       "        0.12, 0.12, 0.12],\n",
       "       [0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16,\n",
       "        0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16,\n",
       "        0.16, 0.16, 0.16],\n",
       "       [0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 ,\n",
       "        0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 ,\n",
       "        0.2 , 0.2 , 0.2 ],\n",
       "       [0.24, 0.24, 0.24, 0.24, 0.24, 0.24, 0.24, 0.24, 0.24, 0.24, 0.24,\n",
       "        0.24, 0.24, 0.24, 0.24, 0.24, 0.24, 0.24, 0.24, 0.24, 0.24, 0.24,\n",
       "        0.24, 0.24, 0.24],\n",
       "       [0.28, 0.28, 0.28, 0.28, 0.28, 0.28, 0.28, 0.28, 0.28, 0.28, 0.28,\n",
       "        0.28, 0.28, 0.28, 0.28, 0.28, 0.28, 0.28, 0.28, 0.28, 0.28, 0.28,\n",
       "        0.28, 0.28, 0.28],\n",
       "       [0.32, 0.32, 0.32, 0.32, 0.32, 0.32, 0.32, 0.32, 0.32, 0.32, 0.32,\n",
       "        0.32, 0.32, 0.32, 0.32, 0.32, 0.32, 0.32, 0.32, 0.32, 0.32, 0.32,\n",
       "        0.32, 0.32, 0.32],\n",
       "       [0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36,\n",
       "        0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36,\n",
       "        0.36, 0.36, 0.36],\n",
       "       [0.4 , 0.4 , 0.4 , 0.4 , 0.4 , 0.4 , 0.4 , 0.4 , 0.4 , 0.4 , 0.4 ,\n",
       "        0.4 , 0.4 , 0.4 , 0.4 , 0.4 , 0.4 , 0.4 , 0.4 , 0.4 , 0.4 , 0.4 ,\n",
       "        0.4 , 0.4 , 0.4 ],\n",
       "       [0.44, 0.44, 0.44, 0.44, 0.44, 0.44, 0.44, 0.44, 0.44, 0.44, 0.44,\n",
       "        0.44, 0.44, 0.44, 0.44, 0.44, 0.44, 0.44, 0.44, 0.44, 0.44, 0.44,\n",
       "        0.44, 0.44, 0.44],\n",
       "       [0.48, 0.48, 0.48, 0.48, 0.48, 0.48, 0.48, 0.48, 0.48, 0.48, 0.48,\n",
       "        0.48, 0.48, 0.48, 0.48, 0.48, 0.48, 0.48, 0.48, 0.48, 0.48, 0.48,\n",
       "        0.48, 0.48, 0.48],\n",
       "       [0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52,\n",
       "        0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52,\n",
       "        0.52, 0.52, 0.52],\n",
       "       [0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56,\n",
       "        0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56,\n",
       "        0.56, 0.56, 0.56],\n",
       "       [0.6 , 0.6 , 0.6 , 0.6 , 0.6 , 0.6 , 0.6 , 0.6 , 0.6 , 0.6 , 0.6 ,\n",
       "        0.6 , 0.6 , 0.6 , 0.6 , 0.6 , 0.6 , 0.6 , 0.6 , 0.6 , 0.6 , 0.6 ,\n",
       "        0.6 , 0.6 , 0.6 ],\n",
       "       [0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64,\n",
       "        0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64, 0.64,\n",
       "        0.64, 0.64, 0.64],\n",
       "       [0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68,\n",
       "        0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68,\n",
       "        0.68, 0.68, 0.68],\n",
       "       [0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72,\n",
       "        0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72,\n",
       "        0.72, 0.72, 0.72],\n",
       "       [0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76,\n",
       "        0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76,\n",
       "        0.76, 0.76, 0.76],\n",
       "       [0.8 , 0.8 , 0.8 , 0.8 , 0.8 , 0.8 , 0.8 , 0.8 , 0.8 , 0.8 , 0.8 ,\n",
       "        0.8 , 0.8 , 0.8 , 0.8 , 0.8 , 0.8 , 0.8 , 0.8 , 0.8 , 0.8 , 0.8 ,\n",
       "        0.8 , 0.8 , 0.8 ],\n",
       "       [0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84,\n",
       "        0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84,\n",
       "        0.84, 0.84, 0.84],\n",
       "       [0.88, 0.88, 0.88, 0.88, 0.88, 0.88, 0.88, 0.88, 0.88, 0.88, 0.88,\n",
       "        0.88, 0.88, 0.88, 0.88, 0.88, 0.88, 0.88, 0.88, 0.88, 0.88, 0.88,\n",
       "        0.88, 0.88, 0.88],\n",
       "       [0.92, 0.92, 0.92, 0.92, 0.92, 0.92, 0.92, 0.92, 0.92, 0.92, 0.92,\n",
       "        0.92, 0.92, 0.92, 0.92, 0.92, 0.92, 0.92, 0.92, 0.92, 0.92, 0.92,\n",
       "        0.92, 0.92, 0.92],\n",
       "       [0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96,\n",
       "        0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96,\n",
       "        0.96, 0.96, 0.96],\n",
       "       [1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  ,\n",
       "        1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  ,\n",
       "        1.  , 1.  , 1.  ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = optimize.minimize(ObjFunc,method = 'CG',jac = grad_ObjFunc, x0 = np.repeat(0,nbX+nbY+nbK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: 7.677025691402801\n",
      "     jac: array([ 4.88075801e-06,  5.10780308e-08, -5.40814185e-08, -9.67674359e-09,\n",
      "       -3.27666407e-08, -5.03933471e-07, -1.01204451e-07, -6.63029291e-08,\n",
      "       -6.00233391e-08, -5.64389548e-07,  9.76712940e-07,  1.22520487e-06,\n",
      "        1.24756462e-06,  1.27584707e-06,  6.88245950e-07,  2.75181310e-07,\n",
      "       -1.86412480e-07,  3.47244011e-07,  7.76230370e-07,  4.02940556e-08,\n",
      "       -6.83570777e-07, -1.13239671e-06, -1.48543343e-06, -1.18186292e-06,\n",
      "       -1.98637193e-06,  1.71723629e-06,  8.02963383e-07,  2.59413708e-07,\n",
      "       -1.08526144e-07, -7.80600168e-07, -1.05835123e-06,  4.97217929e-07,\n",
      "        1.18610346e-07,  1.95591308e-06,  2.88970690e-07,  5.54150734e-07,\n",
      "        3.99675416e-07,  2.02151631e-06,  5.66475382e-07,  1.15734675e-06,\n",
      "        2.94606296e-07,  1.68397986e-07,  1.32991993e-07,  1.41378739e-07,\n",
      "       -3.87513667e-08, -2.92367924e-07, -1.25325454e-06, -1.78125071e-06,\n",
      "       -1.47750538e-06, -4.15938391e-07, -1.59006568e-07,  5.70250050e-07,\n",
      "        1.84558603e-08,  7.05821237e-06])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 421\n",
      "     nit: 162\n",
      "    njev: 421\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([ 3.11102542,  3.1942911 ,  3.35898889,  3.65697275,  4.01385288,\n",
      "        4.43474088,  4.69390301,  5.03868869,  5.67779702,  5.82894836,\n",
      "        5.95965023,  5.82470143,  6.07519871,  6.31489561,  6.38473279,\n",
      "        6.75213323,  7.3382807 ,  8.07538348,  9.44777863, 11.01220163,\n",
      "       13.31827783, 16.13181931, 19.71382466, 23.80260303, 28.92590982,\n",
      "        3.3060289 ,  3.43944638,  3.62095975,  3.94450684,  4.31881146,\n",
      "        4.83525862,  5.31413153,  5.77560961,  6.3814156 ,  6.48732659,\n",
      "        6.45792254,  6.28543211,  6.49701562,  6.5526705 ,  6.49530694,\n",
      "        6.99358023,  7.44582699,  8.2771778 ,  9.4068751 , 11.08136243,\n",
      "       13.43639185, 16.07694629, 19.68768474, 23.83746151, 29.01399273,\n",
      "       -1.36756245, -6.4086041 ,  4.60390741, -1.3049116 ])\n",
      "\n",
      "7.677025691402801\n",
      "[-1.36756245 -6.4086041   4.60390741 -1.3049116 ]\n"
     ]
    }
   ],
   "source": [
    "uvlambdahat =  outcome['x']\n",
    "lambdahat = uvlambdahat[(nbX+nbY):(nbX+nbY+nbK)]\n",
    "print(outcome)\n",
    "print(\"\")\n",
    "print(ObjFunc(uvlambdahat))\n",
    "print(lambdahat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.36756245 -6.4086041   4.60390741 -1.3049116 ]\n"
     ]
    }
   ],
   "source": [
    "print(lambdahat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
